{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18185ea6-e07f-4968-8b83-86b2f08d722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc,confusion_matrix,ConfusionMatrixDisplay,f1_score,recall_score,precision_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3ae6e3-284f-41da-93df-e84eedca4aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"ppi.csv\",index_col=0)\n",
    "data = data[data.sequence != 'X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c3c9f7-17c9-4129-a715-bbee30c13124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Hot Encoding for sequence, spliting test and train\n",
    "edata=data\n",
    "cate_features=[4]\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "column_transformer = ColumnTransformer(\n",
    "[('encoder', enc, cate_features)],\n",
    "remainder='passthrough',\n",
    "verbose_feature_names_out=False\n",
    ")\n",
    "edata = column_transformer.fit_transform(edata)\n",
    "encoded_df = pd.DataFrame(edata, columns=column_transformer.get_feature_names_out(list(data.columns)))\n",
    "encoded_df=encoded_df.drop('normalized_hydropathy_index',axis=1)\n",
    "encoded_df.describe()\n",
    "\n",
    "y=encoded_df.uniprot_id\n",
    "y=y.drop_duplicates()\n",
    "y1=y.sample(frac=1)\n",
    "y1.iloc[:45]\n",
    "test_prots1=list(y1.iloc[:45])\n",
    "train_prots1=list(y1.iloc[45:])\n",
    "test1 = encoded_df[encoded_df['uniprot_id'].isin(test_prots1)]\n",
    "train1 = encoded_df[encoded_df['uniprot_id'].isin(train_prots1)]\n",
    "train1.to_csv('train1.csv', index=False)\n",
    "test1.to_csv('test1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b070b5-e4a5-4449-86f9-35aff6ccc12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster sequence feature\n",
    "data = pd.read_csv(\"ppi.csv\",index_col=0)\n",
    "data = data[data.sequence != 'X']\n",
    "cluster_data=data.drop('normalized_hydropathy_index',axis=1)\n",
    "cluster_data[\"sequence\"] = cluster_data[\"sequence\"].mask(cluster_data[\"sequence\"].isin(['A','G','V']),1)\n",
    "cluster_data[\"sequence\"] = cluster_data[\"sequence\"].mask(cluster_data[\"sequence\"].isin(['I','L','F','P']),2)\n",
    "cluster_data[\"sequence\"] = cluster_data[\"sequence\"].mask(cluster_data[\"sequence\"].isin(['Y','M','T','S']),3)\n",
    "cluster_data[\"sequence\"] = cluster_data[\"sequence\"].mask(cluster_data[\"sequence\"].isin(['H','N','Q','W']),4)\n",
    "cluster_data[\"sequence\"] = cluster_data[\"sequence\"].mask(cluster_data[\"sequence\"].isin(['R','K']),5)\n",
    "cluster_data[\"sequence\"] = cluster_data[\"sequence\"].mask(cluster_data[\"sequence\"].isin(['D','E']),6)\n",
    "cluster_data[\"sequence\"] = cluster_data[\"sequence\"].mask(cluster_data[\"sequence\"]=='C',7)\n",
    "\n",
    "test_prots2=list(test1.uniprot_id.drop_duplicates())\n",
    "train_prots2=list(train1.uniprot_id.drop_duplicates())\n",
    "test2 = cluster_data[cluster_data['uniprot_id'].isin(test_prots2)]\n",
    "train2 = cluster_data[cluster_data['uniprot_id'].isin(train_prots2)]\n",
    "train2.to_csv('train2.csv', index=False)\n",
    "test2.to_csv('test2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf22c444-48a6-427b-9d23-fc211c2cca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove sequence feature\n",
    "data = pd.read_csv(\"ppi.csv\",index_col=0)\n",
    "data = data[data.sequence != 'X']\n",
    "removed_data=data\n",
    "removed_data=removed_data.drop(['sequence','normalized_hydropathy_index'],axis=1)\n",
    "test4 = removed_data[removed_data['uniprot_id'].isin(test_prots2)]\n",
    "train4 = removed_data[removed_data['uniprot_id'].isin(train_prots2)]\n",
    "train4.to_csv('train4.csv', index=False)\n",
    "test4.to_csv('test4.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5b0077-6b04-4d15-a338-169d12708b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate 5-folds for cv\n",
    "cv_prots1 = train_prots2[0:37]\n",
    "cv_prots2 = train_prots2[37:74]\n",
    "cv_prots3 = train_prots2[74:111]\n",
    "cv_prots4 = train_prots2[111:147]\n",
    "cv_prots5 = train_prots2[147:]\n",
    "fold1 = train1[train1['uniprot_id'].isin(cv_prots1)]\n",
    "fold2 = train1[train1['uniprot_id'].isin(cv_prots2)]\n",
    "fold3 = train1[train1['uniprot_id'].isin(cv_prots3)]\n",
    "fold4 = train1[train1['uniprot_id'].isin(cv_prots4)]\n",
    "fold5 = train1[train1['uniprot_id'].isin(cv_prots5)]\n",
    "cv_train1 = pd.concat([fold1, fold2,fold3,fold4], axis=0, ignore_index=True)\n",
    "cv_test1 = fold5\n",
    "cv_train2 = pd.concat([fold1, fold2,fold3,fold5], axis=0, ignore_index=True)\n",
    "cv_test2 = fold4\n",
    "cv_train3 = pd.concat([fold1, fold2,fold4,fold5], axis=0, ignore_index=True)\n",
    "cv_test3 = fold3\n",
    "cv_train4 = pd.concat([fold1, fold3,fold4,fold5], axis=0, ignore_index=True)\n",
    "cv_test4 = fold2\n",
    "cv_train5 = pd.concat([fold2, fold3,fold4,fold5], axis=0, ignore_index=True)\n",
    "cv_test5 = fold1\n",
    "cv_train1.to_csv('cv_train1.csv', index=False)\n",
    "cv_test1.to_csv('cv_test1.csv',index=False)\n",
    "cv_train2.to_csv('cv_train2.csv', index=False)\n",
    "cv_test2.to_csv('cv_test2.csv',index=False)\n",
    "cv_train3.to_csv('cv_train3.csv', index=False)\n",
    "cv_test3.to_csv('cv_test3.csv',index=False)\n",
    "cv_train4.to_csv('cv_train4.csv', index=False)\n",
    "cv_test4.to_csv('cv_test4.csv',index=False)\n",
    "cv_train5.to_csv('cv_train5.csv', index=False)\n",
    "cv_test5.to_csv('cv_test5.csv',index=False)\n",
    "\n",
    "fold1 = train2[train2['uniprot_id'].isin(cv_prots1)]\n",
    "fold2 = train2[train2['uniprot_id'].isin(cv_prots2)]\n",
    "fold3 = train2[train2['uniprot_id'].isin(cv_prots3)]\n",
    "fold4 = train2[train2['uniprot_id'].isin(cv_prots4)]\n",
    "fold5 = train2[train2['uniprot_id'].isin(cv_prots5)]\n",
    "cv2_train1 = pd.concat([fold1, fold2,fold3,fold4], axis=0, ignore_index=True)\n",
    "cv2_test1 = fold5\n",
    "cv2_train2 = pd.concat([fold1, fold2,fold3,fold5], axis=0, ignore_index=True)\n",
    "cv2_test2 = fold4\n",
    "cv2_train3 = pd.concat([fold1, fold2,fold4,fold5], axis=0, ignore_index=True)\n",
    "cv2_test3 = fold3\n",
    "cv2_train4 = pd.concat([fold1, fold3,fold4,fold5], axis=0, ignore_index=True)\n",
    "cv2_test4 = fold2\n",
    "cv2_train5 = pd.concat([fold2, fold3,fold4,fold5], axis=0, ignore_index=True)\n",
    "cv2_test5 = fold1\n",
    "cv2_train1.to_csv('cv2_train1.csv', index=False)\n",
    "cv2_test1.to_csv('cv2_test1.csv',index=False)\n",
    "cv2_train2.to_csv('cv2_train2.csv', index=False)\n",
    "cv2_test2.to_csv('cv2_test2.csv',index=False)\n",
    "cv2_train3.to_csv('cv2_train3.csv', index=False)\n",
    "cv2_test3.to_csv('cv2_test3.csv',index=False)\n",
    "cv2_train4.to_csv('cv2_train4.csv', index=False)\n",
    "cv2_test4.to_csv('cv2_test4.csv',index=False)\n",
    "cv2_train5.to_csv('cv2_train5.csv', index=False)\n",
    "cv2_test5.to_csv('cv2_test5.csv',index=False)\n",
    "\n",
    "\n",
    "fold1 = train4[train4['uniprot_id'].isin(cv_prots1)]\n",
    "fold2 = train4[train4['uniprot_id'].isin(cv_prots2)]\n",
    "fold3 = train4[train4['uniprot_id'].isin(cv_prots3)]\n",
    "fold4 = train4[train4['uniprot_id'].isin(cv_prots4)]\n",
    "fold5 = train4[train4['uniprot_id'].isin(cv_prots5)]\n",
    "cv4_train1 = pd.concat([fold1, fold2,fold3,fold4], axis=0, ignore_index=True)\n",
    "cv4_test1 = fold5\n",
    "cv4_train2 = pd.concat([fold1, fold2,fold3,fold5], axis=0, ignore_index=True)\n",
    "cv4_test2 = fold4\n",
    "cv4_train3 = pd.concat([fold1, fold2,fold4,fold5], axis=0, ignore_index=True)\n",
    "cv4_test3 = fold3\n",
    "cv4_train4 = pd.concat([fold1, fold3,fold4,fold5], axis=0, ignore_index=True)\n",
    "cv4_test4 = fold2\n",
    "cv4_train5 = pd.concat([fold2, fold3,fold4,fold5], axis=0, ignore_index=True)\n",
    "cv4_test5 = fold1\n",
    "cv4_train1.to_csv('cv4_train1.csv', index=False)\n",
    "cv4_test1.to_csv('cv4_test1.csv',index=False)\n",
    "cv4_train2.to_csv('cv4_train2.csv', index=False)\n",
    "cv4_test2.to_csv('cv4_test2.csv',index=False)\n",
    "cv4_train3.to_csv('cv4_train3.csv', index=False)\n",
    "cv4_test3.to_csv('cv4_test3.csv',index=False)\n",
    "cv4_train4.to_csv('cv4_train4.csv', index=False)\n",
    "cv4_test4.to_csv('cv4_test4.csv',index=False)\n",
    "cv4_train5.to_csv('cv4_train5.csv', index=False)\n",
    "cv4_test5.to_csv('cv4_test5.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6814cb36-f9e0-4e5c-aae5-ffd579422a70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6b59e8-2364-4a4d-9f6e-c7f3d001d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUS for cv\n",
    "cv_train1 = pd.read_csv(\"cv_train1.csv\")\n",
    "cv_train2 = pd.read_csv(\"cv_train2.csv\")\n",
    "cv_train3 = pd.read_csv(\"cv_train3.csv\")\n",
    "cv_train4 = pd.read_csv(\"cv_train4.csv\")\n",
    "cv_train5 = pd.read_csv(\"cv_train5.csv\")\n",
    "\n",
    "x = cv_train1.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y = cv_train1.p_interface\n",
    "rus = RandomUnderSampler(random_state=36)\n",
    "rus_x,rus_y = rus.fit_resample(x, y)\n",
    "print('Resampled dataset shape %s' % Counter(rus_y))\n",
    "import pickle\n",
    "with open(\"RUScv1.pickle\", \"wb\") as file:\n",
    "    pickle.dump((rus_x, rus_y), file)\n",
    "\n",
    "x = cv_train2.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y = cv_train2.p_interface\n",
    "rus = RandomUnderSampler(random_state=36)\n",
    "rus_x,rus_y = rus.fit_resample(x, y)\n",
    "print('Resampled dataset shape %s' % Counter(rus_y))\n",
    "import pickle\n",
    "with open(\"RUScv2.pickle\", \"wb\") as file:\n",
    "    pickle.dump((rus_x, rus_y), file)\n",
    "\n",
    "x = cv_train3.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y = cv_train3.p_interface\n",
    "rus = RandomUnderSampler(random_state=36)\n",
    "rus_x,rus_y = rus.fit_resample(x, y)\n",
    "print('Resampled dataset shape %s' % Counter(rus_y))\n",
    "import pickle\n",
    "with open(\"RUScv3.pickle\", \"wb\") as file:\n",
    "    pickle.dump((rus_x, rus_y), file)\n",
    "\n",
    "x = cv_train4.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y = cv_train4.p_interface\n",
    "rus = RandomUnderSampler(random_state=36)\n",
    "rus_x,rus_y = rus.fit_resample(x, y)\n",
    "print('Resampled dataset shape %s' % Counter(rus_y))\n",
    "import pickle\n",
    "with open(\"RUScv4.pickle\", \"wb\") as file:\n",
    "    pickle.dump((rus_x, rus_y), file)\n",
    "\n",
    "x = cv_train5.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y = cv_train5.p_interface\n",
    "rus = RandomUnderSampler(random_state=36)\n",
    "rus_x,rus_y = rus.fit_resample(x, y)\n",
    "print('Resampled dataset shape %s' % Counter(rus_y))\n",
    "import pickle\n",
    "with open(\"RUScv5.pickle\", \"wb\") as file:\n",
    "    pickle.dump((rus_x, rus_y), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa9feba-33e7-47b9-9b30-d65fd8770d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_train1 = pd.read_csv(\"cv2_train1.csv\")\n",
    "cv_train2 = pd.read_csv(\"cv2_train2.csv\")\n",
    "cv_train3 = pd.read_csv(\"cv2_train3.csv\")\n",
    "cv_train4 = pd.read_csv(\"cv2_train4.csv\")\n",
    "cv_train5 = pd.read_csv(\"cv2_train5.csv\")\n",
    "x = cv_train1.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y = cv_train1.p_interface\n",
    "rus = RandomUnderSampler(random_state=36)\n",
    "rus_x,rus_y = rus.fit_resample(x, y)\n",
    "print('Resampled dataset shape %s' % Counter(rus_y))\n",
    "import pickle\n",
    "with open(\"RUScv21.pickle\", \"wb\") as file:\n",
    "    pickle.dump((rus_x, rus_y), file)\n",
    "\n",
    "x = cv_train2.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y = cv_train2.p_interface\n",
    "rus = RandomUnderSampler(random_state=36)\n",
    "rus_x,rus_y = rus.fit_resample(x, y)\n",
    "print('Resampled dataset shape %s' % Counter(rus_y))\n",
    "import pickle\n",
    "with open(\"RUScv22.pickle\", \"wb\") as file:\n",
    "    pickle.dump((rus_x, rus_y), file)\n",
    "\n",
    "x = cv_train3.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y = cv_train3.p_interface\n",
    "rus = RandomUnderSampler(random_state=36)\n",
    "rus_x,rus_y = rus.fit_resample(x, y)\n",
    "print('Resampled dataset shape %s' % Counter(rus_y))\n",
    "import pickle\n",
    "with open(\"RUScv23.pickle\", \"wb\") as file:\n",
    "    pickle.dump((rus_x, rus_y), file)\n",
    "\n",
    "x = cv_train4.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y = cv_train4.p_interface\n",
    "rus = RandomUnderSampler(random_state=36)\n",
    "rus_x,rus_y = rus.fit_resample(x, y)\n",
    "print('Resampled dataset shape %s' % Counter(rus_y))\n",
    "import pickle\n",
    "with open(\"RUScv24.pickle\", \"wb\") as file:\n",
    "    pickle.dump((rus_x, rus_y), file)\n",
    "\n",
    "x = cv_train5.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y = cv_train5.p_interface\n",
    "rus = RandomUnderSampler(random_state=36)\n",
    "rus_x,rus_y = rus.fit_resample(x, y)\n",
    "print('Resampled dataset shape %s' % Counter(rus_y))\n",
    "import pickle\n",
    "with open(\"RUScv25.pickle\", \"wb\") as file:\n",
    "    pickle.dump((rus_x, rus_y), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a875d49e-8be9-4a15-b97c-361a01d1321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_train1 = pd.read_csv(\"cv4_train1.csv\")\n",
    "cv_train2 = pd.read_csv(\"cv4_train2.csv\")\n",
    "cv_train3 = pd.read_csv(\"cv4_train3.csv\")\n",
    "cv_train4 = pd.read_csv(\"cv4_train4.csv\")\n",
    "cv_train5 = pd.read_csv(\"cv4_train5.csv\")\n",
    "x = cv_train1.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y = cv_train1.p_interface\n",
    "rus = RandomUnderSampler(random_state=36)\n",
    "rus_x,rus_y = rus.fit_resample(x, y)\n",
    "print('Resampled dataset shape %s' % Counter(rus_y))\n",
    "import pickle\n",
    "with open(\"RUScv41.pickle\", \"wb\") as file:\n",
    "    pickle.dump((rus_x, rus_y), file)\n",
    "\n",
    "x = cv_train2.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y = cv_train2.p_interface\n",
    "rus = RandomUnderSampler(random_state=36)\n",
    "rus_x,rus_y = rus.fit_resample(x, y)\n",
    "print('Resampled dataset shape %s' % Counter(rus_y))\n",
    "import pickle\n",
    "with open(\"RUScv42.pickle\", \"wb\") as file:\n",
    "    pickle.dump((rus_x, rus_y), file)\n",
    "\n",
    "x = cv_train3.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y = cv_train3.p_interface\n",
    "rus = RandomUnderSampler(random_state=36)\n",
    "rus_x,rus_y = rus.fit_resample(x, y)\n",
    "print('Resampled dataset shape %s' % Counter(rus_y))\n",
    "import pickle\n",
    "with open(\"RUScv43.pickle\", \"wb\") as file:\n",
    "    pickle.dump((rus_x, rus_y), file)\n",
    "\n",
    "x = cv_train4.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y = cv_train4.p_interface\n",
    "rus = RandomUnderSampler(random_state=36)\n",
    "rus_x,rus_y = rus.fit_resample(x, y)\n",
    "print('Resampled dataset shape %s' % Counter(rus_y))\n",
    "import pickle\n",
    "with open(\"RUScv44.pickle\", \"wb\") as file:\n",
    "    pickle.dump((rus_x, rus_y), file)\n",
    "\n",
    "x = cv_train5.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y = cv_train5.p_interface\n",
    "rus = RandomUnderSampler(random_state=36)\n",
    "rus_x,rus_y = rus.fit_resample(x, y)\n",
    "print('Resampled dataset shape %s' % Counter(rus_y))\n",
    "import pickle\n",
    "with open(\"RUScv45.pickle\", \"wb\") as file:\n",
    "    pickle.dump((rus_x, rus_y), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c7ebe0-81ef-4d67-830a-be752641a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smote for cv\n",
    "x = cv_train1.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y = cv_train1.p_interface\n",
    "smote = SMOTE(random_state=42)\n",
    "smote1_x,smote1_y = smote.fit_resample(x,y)\n",
    "print('Resampled dataset shape %s' % Counter(smote1_y))\n",
    "\n",
    "x = cv_train2.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y = cv_train2.p_interface\n",
    "smote = SMOTE(random_state=42)\n",
    "smote2_x,smote2_y = smote.fit_resample(x,y)\n",
    "print('Resampled dataset shape %s' % Counter(smote2_y))\n",
    "\n",
    "x = cv_train3.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y = cv_train3.p_interface\n",
    "smote = SMOTE(random_state=42)\n",
    "smote3_x,smote3_y = smote.fit_resample(x,y)\n",
    "print('Resampled dataset shape %s' % Counter(smote3_y))\n",
    "\n",
    "x = cv_train4.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y = cv_train4.p_interface\n",
    "smote = SMOTE(random_state=42)\n",
    "smote4_x,smote4_y = smote.fit_resample(x,y)\n",
    "print('Resampled dataset shape %s' % Counter(smote4_y))\n",
    "\n",
    "x = cv_train5.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y = cv_train5.p_interface\n",
    "smote = SMOTE(random_state=42)\n",
    "smote5_x,smote5_y = smote.fit_resample(x,y)\n",
    "print('Resampled dataset shape %s' % Counter(smote5_y))\n",
    "\n",
    "with open(\"SMOTE.pickle\", \"wb\") as file:\n",
    "    pickle.dump((smote1_x, smote1_y,smote2_x,smote2_y,smote3_x,smote3_y,smote4_x,smote4_y,smote5_x,smote5_y), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a139d40c-207b-404f-bb87-85945649a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUS for whole train set(used for final model)\n",
    "train1 = pd.read_csv(\"train1.csv\")\n",
    "x = train1.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y = train1.p_interface\n",
    "rus = RandomUnderSampler(random_state=36)\n",
    "rus_x,rus_y = rus.fit_resample(x, y)\n",
    "rus_x.to_csv('rus_x.csv', index=False)\n",
    "rus_y.to_csv('rus_y.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbb4667-64bd-4c70-a86c-6af1f7c22de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data for hyperparameter tuning\n",
    "cv_train1 = pd.read_csv(\"cv_train1.csv\")\n",
    "x_train1 = cv_train1.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y_train1 = cv_train1.p_interface\n",
    "\n",
    "cv_train2 = pd.read_csv(\"cv_train2.csv\")\n",
    "x_train2 = cv_train2.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y_train2 = cv_train2.p_interface\n",
    "\n",
    "cv_train3 = pd.read_csv(\"cv_train3.csv\")\n",
    "x_train3 = cv_train3.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y_train3 = cv_train3.p_interface\n",
    "\n",
    "cv_train4 = pd.read_csv(\"cv_train4.csv\")\n",
    "x_train4 = cv_train4.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y_train4 = cv_train4.p_interface\n",
    "\n",
    "cv_train5 = pd.read_csv(\"cv_train5.csv\")\n",
    "x_train5 = cv_train5.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y_train5 = cv_train5.p_interface\n",
    "\n",
    "cv_test1 = pd.read_csv(\"cv_test1.csv\")\n",
    "cv_test2 = pd.read_csv(\"cv_test2.csv\")\n",
    "cv_test3 = pd.read_csv(\"cv_test3.csv\")\n",
    "cv_test4 = pd.read_csv(\"cv_test4.csv\")\n",
    "cv_test5 = pd.read_csv(\"cv_test5.csv\")\n",
    "\n",
    "x_test1 = cv_test1.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y_test1 = cv_test1.p_interface\n",
    "x_test2 = cv_test2.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y_test2 = cv_test2.p_interface\n",
    "x_test3 = cv_test3.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y_test3 = cv_test3.p_interface\n",
    "x_test4 = cv_test4.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y_test4 = cv_test4.p_interface\n",
    "x_test5 = cv_test5.drop(['aa_ProtPosition', 'domain','p_interface','uniprot_id','Rlength'], axis=1)\n",
    "y_test5 = cv_test5.p_interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385dfdf0-817e-4ba1-8f8f-3071ca281daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search(same setting for all except final model)\n",
    "for i in [100,120,140,160]:\n",
    "    for j in [10,15,20]:\n",
    "        rf = RandomForestClassifier(random_state=0,n_estimators=i,max_depth=j,n_jobs=-1)\n",
    "        rf.fit(x_train1, y_train1)\n",
    "        rf_predictions = rf.predict(x_test1)\n",
    "        f1_1 = f1_score(y_test1,rf_predictions)\n",
    "        acc_1 = accuracy_score(y_test1,rf_predictions)\n",
    "        y_rf_prob = rf.predict_proba(x_test1)[:, 1]\n",
    "        roc_rf = roc_curve(y_test1, y_rf_prob)\n",
    "        auc_1 = auc(roc_rf[0], roc_rf[1])\n",
    "        \n",
    "        rf = RandomForestClassifier(random_state=0,n_estimators=i,max_depth=j,n_jobs=-1)\n",
    "        rf.fit(x_train2, y_train2)\n",
    "        rf_predictions = rf.predict(x_test2)\n",
    "        f1_2 = f1_score(y_test2,rf_predictions)\n",
    "        acc_2 = accuracy_score(y_test2,rf_predictions)\n",
    "        y_rf_prob = rf.predict_proba(x_test2)[:, 1]\n",
    "        roc_rf = roc_curve(y_test2, y_rf_prob)\n",
    "        auc_2 = auc(roc_rf[0], roc_rf[1])\n",
    "        \n",
    "        rf = RandomForestClassifier(random_state=0,n_estimators=i,max_depth=j,n_jobs=-1)\n",
    "        rf.fit(x_train3, y_train3)\n",
    "        rf_predictions = rf.predict(x_test3)\n",
    "        f1_3 = f1_score(y_test3,rf_predictions)\n",
    "        acc_3 = accuracy_score(y_test3,rf_predictions)\n",
    "        y_rf_prob = rf.predict_proba(x_test3)[:, 1]\n",
    "        roc_rf = roc_curve(y_test3, y_rf_prob)\n",
    "        auc_3 = auc(roc_rf[0], roc_rf[1])\n",
    "        \n",
    "        rf = RandomForestClassifier(random_state=0,n_estimators=i,max_depth=j,n_jobs=-1)\n",
    "        rf.fit(x_train4, y_train4)\n",
    "        rf_predictions = rf.predict(x_test4)\n",
    "        f1_4 = f1_score(y_test4,rf_predictions)\n",
    "        acc_4 = accuracy_score(y_test4,rf_predictions)\n",
    "        y_rf_prob = rf.predict_proba(x_test4)[:, 1]\n",
    "        roc_rf = roc_curve(y_test4, y_rf_prob)\n",
    "        auc_4 = auc(roc_rf[0], roc_rf[1])\n",
    "        \n",
    "        rf = RandomForestClassifier(random_state=0,n_estimators=i,max_depth=j,n_jobs=-1)\n",
    "        rf.fit(x_train5, y_train5)\n",
    "        rf_predictions = rf.predict(x_test5)\n",
    "        f1_5 = f1_score(y_test5,rf_predictions)\n",
    "        acc_5 = accuracy_score(y_test5,rf_predictions)\n",
    "        y_rf_prob = rf.predict_proba(x_test5)[:, 1]\n",
    "        roc_rf = roc_curve(y_test5, y_rf_prob)\n",
    "        auc_5 = auc(roc_rf[0], roc_rf[1])\n",
    "        \n",
    "        print(i,j,(f1_1+f1_2+f1_3+f1_4+f1_5)/5,(acc_1+acc_2+acc_3+acc_4+acc_5)/5,(auc_1+auc_2+auc_3+auc_4+auc_5)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406aa1d7-7642-4113-b0ec-db403b3ddf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation of feature engineering\n",
    "#original sequence\n",
    "with open(\"RUScv1.pickle\", \"rb\") as file:\n",
    "    rus1_x, rus1_y = pickle.load(file)\n",
    "\n",
    "with open(\"RUScv2.pickle\", \"rb\") as file:\n",
    "    rus2_x, rus2_y = pickle.load(file)\n",
    "\n",
    "with open(\"RUScv3.pickle\", \"rb\") as file:\n",
    "    rus3_x, rus3_y = pickle.load(file)\n",
    "\n",
    "with open(\"RUScv4.pickle\", \"rb\") as file:\n",
    "    rus4_x, rus4_y = pickle.load(file)\n",
    "\n",
    "with open(\"RUScv5.pickle\", \"rb\") as file:\n",
    "    rus5_x, rus5_y = pickle.load(file)\n",
    "for i in [100,120,140,160]:\n",
    "    for j in [10,15,20]:\n",
    "        rf = RandomForestClassifier(random_state=0,n_estimators=i,max_depth=j,n_jobs=-1)\n",
    "        rf.fit(rus1_x, rus1_y)\n",
    "        rf_predictions = rf.predict(x_test1)\n",
    "        f1_1 = f1_score(y_test1,rf_predictions)\n",
    "        rf.fit(rus2_x, rus2_y)\n",
    "        rf_predictions = rf.predict(x_test2)\n",
    "        f1_2 = f1_score(y_test2,rf_predictions)\n",
    "        rf.fit(rus3_x, rus3_y)\n",
    "        rf_predictions = rf.predict(x_test3)\n",
    "        f1_3 = f1_score(y_test3,rf_predictions)\n",
    "        rf.fit(rus4_x, rus4_y)\n",
    "        rf_predictions = rf.predict(x_test4)\n",
    "        f1_4 = f1_score(y_test4,rf_predictions)\n",
    "        rf.fit(rus5_x, rus5_y)\n",
    "        rf_predictions = rf.predict(x_test5)\n",
    "        f1_5 = f1_score(y_test5,rf_predictions)\n",
    "        \n",
    "        print(i,j,(f1_1+f1_2+f1_3+f1_4+f1_5)/5)\n",
    "#cluster sequence\n",
    "with open(\"RUScv21.pickle\", \"rb\") as file:\n",
    "    rus1_x, rus1_y = pickle.load(file)\n",
    "\n",
    "with open(\"RUScv22.pickle\", \"rb\") as file:\n",
    "    rus2_x, rus2_y = pickle.load(file)\n",
    "\n",
    "with open(\"RUScv23.pickle\", \"rb\") as file:\n",
    "    rus3_x, rus3_y = pickle.load(file)\n",
    "\n",
    "with open(\"RUScv24.pickle\", \"rb\") as file:\n",
    "    rus4_x, rus4_y = pickle.load(file)\n",
    "\n",
    "with open(\"RUScv25.pickle\", \"rb\") as file:\n",
    "    rus5_x, rus5_y = pickle.load(file)\n",
    "for i in [100,120,140,160]:\n",
    "    for j in [10,15,20]:\n",
    "        rf = RandomForestClassifier(random_state=0,n_estimators=i,max_depth=j,n_jobs=-1)\n",
    "        rf.fit(rus1_x, rus1_y)\n",
    "        rf_predictions = rf.predict(x_test1)\n",
    "        f1_1 = f1_score(y_test1,rf_predictions)\n",
    "        rf.fit(rus2_x, rus2_y)\n",
    "        rf_predictions = rf.predict(x_test2)\n",
    "        f1_2 = f1_score(y_test2,rf_predictions)\n",
    "        rf.fit(rus3_x, rus3_y)\n",
    "        rf_predictions = rf.predict(x_test3)\n",
    "        f1_3 = f1_score(y_test3,rf_predictions)\n",
    "        rf.fit(rus4_x, rus4_y)\n",
    "        rf_predictions = rf.predict(x_test4)\n",
    "        f1_4 = f1_score(y_test4,rf_predictions)\n",
    "        rf.fit(rus5_x, rus5_y)\n",
    "        rf_predictions = rf.predict(x_test5)\n",
    "        f1_5 = f1_score(y_test5,rf_predictions)\n",
    "        \n",
    "        print(i,j,(f1_1+f1_2+f1_3+f1_4+f1_5)/5)\n",
    "#remove sequence\n",
    "with open(\"RUScv41.pickle\", \"rb\") as file:\n",
    "    rus1_x, rus1_y = pickle.load(file)\n",
    "\n",
    "with open(\"RUScv42.pickle\", \"rb\") as file:\n",
    "    rus2_x, rus2_y = pickle.load(file)\n",
    "\n",
    "with open(\"RUScv43.pickle\", \"rb\") as file:\n",
    "    rus3_x, rus3_y = pickle.load(file)\n",
    "\n",
    "with open(\"RUScv44.pickle\", \"rb\") as file:\n",
    "    rus4_x, rus4_y = pickle.load(file)\n",
    "\n",
    "with open(\"RUScv45.pickle\", \"rb\") as file:\n",
    "    rus5_x, rus5_y = pickle.load(file)\n",
    "for i in [100,120,140,160]:\n",
    "    for j in [10,15,20]:\n",
    "        rf = RandomForestClassifier(random_state=0,n_estimators=i,max_depth=j,n_jobs=-1)\n",
    "        rf.fit(rus1_x, rus1_y)\n",
    "        rf_predictions = rf.predict(x_test1)\n",
    "        f1_1 = f1_score(y_test1,rf_predictions)\n",
    "        rf.fit(rus2_x, rus2_y)\n",
    "        rf_predictions = rf.predict(x_test2)\n",
    "        f1_2 = f1_score(y_test2,rf_predictions)\n",
    "        rf.fit(rus3_x, rus3_y)\n",
    "        rf_predictions = rf.predict(x_test3)\n",
    "        f1_3 = f1_score(y_test3,rf_predictions)\n",
    "        rf.fit(rus4_x, rus4_y)\n",
    "        rf_predictions = rf.predict(x_test4)\n",
    "        f1_4 = f1_score(y_test4,rf_predictions)\n",
    "        rf.fit(rus5_x, rus5_y)\n",
    "        rf_predictions = rf.predict(x_test5)\n",
    "        f1_5 = f1_score(y_test5,rf_predictions)\n",
    "        \n",
    "        print(i,j,(f1_1+f1_2+f1_3+f1_4+f1_5)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398c662a-857d-48a3-964d-15db2d01e38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recursive feature elimination\n",
    "x = pd.read_csv(\"rus_x.csv\")\n",
    "y = pd.read_csv(\"rus_y.csv\")\n",
    "x=x.drop('normalized_hydropathy_index',axis=1)\n",
    "y=y.to_numpy().ravel()\n",
    "features=[]\n",
    "scores=[]\n",
    "for i in range(150):\n",
    "    rf=RandomForestClassifier(random_state=0,n_estimators=140,max_depth=10,n_jobs=-1)\n",
    "    rf.fit(x,y)\n",
    "    feature_importance = zip(rf.feature_names_in_,rf.feature_importances_)\n",
    "    features.append(rf.feature_names_in_)\n",
    "    rf.fit(rus1_x, rus1_y)\n",
    "    rf_predictions = rf.predict(x_test1)\n",
    "    f1_1 = f1_score(y_test1,rf_predictions)\n",
    "    rf.fit(rus2_x, rus2_y)\n",
    "    rf_predictions = rf.predict(x_test2)\n",
    "    f1_2 = f1_score(y_test2,rf_predictions)\n",
    "    rf.fit(rus3_x, rus3_y)\n",
    "    rf_predictions = rf.predict(x_test3)\n",
    "    f1_3 = f1_score(y_test3,rf_predictions)\n",
    "    rf.fit(rus4_x, rus4_y)\n",
    "    rf_predictions = rf.predict(x_test4)\n",
    "    f1_4 = f1_score(y_test4,rf_predictions)\n",
    "    rf.fit(rus5_x, rus5_y)\n",
    "    rf_predictions = rf.predict(x_test5)\n",
    "    f1_5 = f1_score(y_test5,rf_predictions)\n",
    "    mean_f=(f1_1+f1_2+f1_3+f1_4+f1_5)/5\n",
    "    scores.append(mean_f)\n",
    "    lst =list(feature_importance)\n",
    "    lst = dict(lst)\n",
    "    min_key = min(lst, key=lst.get)\n",
    "    if len(x.columns)>1:\n",
    "        x=x.drop(min_key,axis=1)\n",
    "        rus1_x=rus1_x.drop(min_key,axis=1)\n",
    "        rus2_x=rus2_x.drop(min_key,axis=1)\n",
    "        rus3_x=rus3_x.drop(min_key,axis=1)\n",
    "        rus4_x=rus4_x.drop(min_key,axis=1)\n",
    "        rus5_x=rus5_x.drop(min_key,axis=1)\n",
    "        x_test1=x_test1.drop(min_key,axis=1)\n",
    "        x_test2=x_test2.drop(min_key,axis=1)\n",
    "        x_test3=x_test3.drop(min_key,axis=1)\n",
    "        x_test4=x_test4.drop(min_key,axis=1)\n",
    "        x_test5=x_test5.drop(min_key,axis=1)\n",
    "\n",
    "number_of_features=[i for i in range(150,0,-1)]\n",
    "plt.plot(number_of_features,scores)\n",
    "plt.xlabel(\"number of features\")\n",
    "plt.ylabel(\"mean f1 score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bd7103-dc9a-4ad6-a0ee-67eef2d81468",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final model training and testing\n",
    "x = pd.read_csv(\"rus_x.csv\")\n",
    "y = pd.read_csv(\"rus_y.csv\")\n",
    "y=y.to_numpy().ravel()\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "params = {'n_estimators': 140, 'max_depth': 10, 'criterion': 'gini'}\n",
    "rf.set_params(**params)\n",
    "\n",
    "rf.fit(x,y)\n",
    "rf_predictions = rf.predict(x_test1)\n",
    "print(\"Accuracy of Random Forest:\", accuracy_score(y_test1, rf_predictions))\n",
    "y_rf_prob = rf.predict_proba(x_test1)[:, 1]\n",
    "roc_rf = roc_curve(y_test1, y_rf_prob)\n",
    "print(\"AUC of Random Forest:\", auc(roc_rf[0], roc_rf[1]))\n",
    "f1 = f1_score(y_test1,rf_predictions)\n",
    "print(\"f1 score of Random Forest:\",f1)\n",
    "cm = confusion_matrix(y_test1, rf_predictions, labels=rf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=rf.classes_)\n",
    "disp.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdc17db-33fe-4713-80ba-f3e4fdbbb829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature importance\n",
    "feature_importance = zip(rf.feature_names_in_,rf.feature_importances_)\n",
    "lst =list(feature_importance)\n",
    "lst = dict(lst)\n",
    "keys = list(lst.keys())\n",
    "vals = list(lst.values())\n",
    "\n",
    "lst['wm_normalized_abs_surf_acc'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].endswith('wm_normalized_abs_surf_acc'):\n",
    "        lst['wm_normalized_abs_surf_acc'] += lst[keys[i]]\n",
    "\n",
    "\n",
    "lst['wm_normalized_hydropathy_index'] = 0\n",
    "for i in  range(len(keys)):\n",
    "    if keys[i].endswith('wm_normalized_hydropathy_index'):\n",
    "        lst['wm_normalized_hydropathy_index'] += lst[keys[i]]\n",
    "\n",
    "\n",
    "lst['wm_rel_surf_acc'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].endswith('wm_rel_surf_acc'):\n",
    "        lst['wm_rel_surf_acc'] += lst[keys[i]]\n",
    "\n",
    "\n",
    "lst['wm_prob_sheet'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].endswith('wm_prob_sheet'):\n",
    "        lst['wm_prob_sheet'] += lst[keys[i]]\n",
    "\n",
    "\n",
    "lst['wm_prob_helix'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].endswith('wm_prob_helix'):\n",
    "        lst['wm_prob_helix'] += lst[keys[i]]\n",
    "\n",
    "\n",
    "lst['wm_prob_coil'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].endswith('wm_prob_coil'):\n",
    "        lst['wm_prob_coil'] += lst[keys[i]]\n",
    "\n",
    "\n",
    "lst['wm_pssm_A'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].endswith('wm_pssm_A'):\n",
    "        lst['wm_pssm_A'] += lst[keys[i]]\n",
    "\n",
    "\n",
    "lst['wm_pssm_R'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].endswith('wm_pssm_R'):\n",
    "        lst['wm_pssm_R'] += lst[keys[i]]\n",
    "\n",
    "\n",
    "lst['wm_pssm_N'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].endswith('wm_pssm_N'):\n",
    "        lst['wm_pssm_N'] += lst[keys[i]]\n",
    "\n",
    "\n",
    "lst['wm_pssm_D'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].endswith('wm_pssm_D'):\n",
    "        lst['wm_pssm_D'] += lst[keys[i]]\n",
    "\n",
    "\n",
    "lst['wm_pssm_C'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].endswith('wm_pssm_C'):\n",
    "        lst['wm_pssm_C'] += lst[keys[i]]\n",
    "\n",
    "\n",
    "lst['wm_pssm_Q'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].endswith('wm_pssm_Q'):\n",
    "        lst['wm_pssm_Q'] += lst[keys[i]]\n",
    "\n",
    "\n",
    "lst['wm_pssm_E'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].endswith('wm_pssm_E'):\n",
    "        lst['wm_pssm_E'] += lst[keys[i]]\n",
    "\n",
    "\n",
    "lst['wm_pssm_G'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].endswith('wm_pssm_G'):\n",
    "        lst['wm_pssm_G'] += lst[keys[i]]\n",
    "\n",
    "\n",
    "lst['wm_pssm_H'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].endswith('wm_pssm_H'):\n",
    "        lst['wm_pssm_H'] += lst[keys[i]]\n",
    "\n",
    "\n",
    "lst['wm_pssm_I'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].endswith('wm_pssm_I'):\n",
    "        lst['wm_pssm_I'] += lst[keys[i]]\n",
    "\n",
    "\n",
    "lst['wm_pssm_L'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].endswith('wm_pssm_L'):\n",
    "        lst['wm_pssm_L'] += lst[keys[i]]\n",
    "\n",
    "\n",
    "lst['wm_pssm_K'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].endswith('wm_pssm_K'):\n",
    "        lst['wm_pssm_K'] += lst[keys[i]]\n",
    "\n",
    "\n",
    "lst['wm_pssm_M'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].endswith('wm_pssm_M'):\n",
    "        lst['wm_pssm_M'] += lst[keys[i]]\n",
    "\n",
    "\n",
    "lst['wm_pssm_F'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].endswith('wm_pssm_F'):\n",
    "        lst['wm_pssm_F'] += lst[keys[i]]\n",
    "\n",
    "\n",
    "lst['wm_pssm_P'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].endswith('wm_pssm_P'):\n",
    "        lst['wm_pssm_P'] += lst[keys[i]]\n",
    "\n",
    "\n",
    "lst['wm_pssm_S'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].endswith('wm_pssm_S'):\n",
    "        lst['wm_pssm_S'] += lst[keys[i]]\n",
    "\n",
    "\n",
    "lst['wm_pssm_T'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].endswith('wm_pssm_T'):\n",
    "        lst['wm_pssm_T'] += lst[keys[i]]\n",
    "\n",
    "\n",
    "lst['wm_pssm_W'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].endswith('wm_pssm_W'):\n",
    "        lst['wm_pssm_W'] += lst[keys[i]]\n",
    "\n",
    "\n",
    "lst['wm_pssm_Y'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].endswith('wm_pssm_Y'):\n",
    "        lst['wm_pssm_Y'] += lst[keys[i]]\n",
    "\n",
    "\n",
    "lst['wm_pssm_V'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].endswith('wm_pssm_V'):\n",
    "        lst['wm_pssm_V'] += lst[keys[i]]\n",
    "\n",
    "lst['sum_sequence'] = 0\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].startswith('sequence'):\n",
    "        lst['sum_sequence'] += lst[keys[i]]\n",
    "\n",
    "for i in range(len(keys)):\n",
    "    if keys[i].startswith('3_wm') or keys[i].startswith('5_wm') or keys[i].startswith('7_wm') or keys[i].startswith('9_wm') or keys[i].startswith('sequence'):\n",
    "        del lst[keys[i]]\n",
    "\n",
    "lst[\"sequence\"] = lst.pop(\"sum_sequence\")\n",
    "\n",
    "names = list(lst.keys())\n",
    "importance = list(lst.values())\n",
    "sorted_idx = np.argsort(importance)\n",
    "fig = plt.figure(figsize=(9, 11))\n",
    "plt.barh(range(len(sorted_idx)), np.array(importance)[sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), np.array(names)[sorted_idx])\n",
    "plt.title('Feature Importance of Final Model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
